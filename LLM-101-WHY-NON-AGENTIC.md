# LLM 101: Why non-agentic?

## Introduction

Large language model is a pattern matcher that is created in two phases: pre-training and post-training. It first learns to mimic original text (e.g. a book) by predicting next token (e.g. a word) in a sequence (e.g. a sentence). Once it can coherently continue given inputs (_base_ model is ready), it's time for post-training (creating _instruct_ model). This relatively quick process tunes the model on question & answer pairs (e.g. exam tasks, Stack Overflow posts) making it useful for human interaction.

What determines the model's capabilities are mainly its pre-training dataset, computing power and duration of the training process.

Nowadays, when creating datasets, AI labs don't limit themselves to human-made books, codeâ€”they use current models to generate artificial (synthetic) data based on evaluated needs.

With the goal of getting correct pattern match to user's question, generating synthetic data is mostly about talking about the same things in many different ways.

Perception of the model's intelligence is a brilliant illusion made when it is able to match pattern seen in its dataset.

## The agentic illusion

A coding agent is a system that is designed to use tools (e.g. file system) to complete tasks. It is an attempt to give the model the ability to plan, execute, and iterate on its own.

The agent's workflow is usually:

1. **Plan:** Break down the task into smaller steps.
2. **Execute:** Use tools to perform a step.
3. **Observe:** Analyze the output of the tool.
4. **Iterate:** Repeat steps 1-3 until the task is complete or the maximum number of iterations is reached.

This approach is inherently flawed because the model is still just a pattern matcher. It is matching the pattern of "how an agent should behave" based on its training data, which includes synthetic data generated by other LLMs simulating agent behavior. When the task deviates slightly from the patterns it has learned, the agent often fails spectacularly in a loop of hallucinated tool usage and self-correction failures.

The core issue is that the model is not reasoning; it is simulating reasoning based on observed, defined set of examples. The more complex the task, the more likely it will stray into unseen teritory and break down.

## Non-agentic nature of the tool

CWC is designed to leverage the LLM's core strength: pattern matching for code generation.

Instead of trying to simulate complex planning and tool usage (which is prone to failure), CWC focuses on providing the best possible input (context, prompt, explicit edit format instructions) and then reliably integrating the output.

The user acts as the orchestrator, defining the scope and intent, while the LLM focuses solely on generating the required code changes (diffs, whole files, or truncated blocks). This approach avoids the inherent instability and hallucination loops associated with agentic systems, resulting in a more reliable and predictable coding assistant that keeps you in control.
