## LLM 101

Large language model is a pattern matcher that is created in two phases: pre-training and post-training. It first learns to mimic original text (e.g. a book) by predicting next token (e.g. a word) in a sequence (e.g. a sentence). Once it can coherently continue given inputs (_base_ model is ready), it's time for post-training (creating _instruct_ model). This relatively quick process tunes the model on question & answer pairs (e.g. exam tasks, Stack Overflow posts) making it useful for human interaction.

What determines the model's capabilies are mainly its pre-training dataset, computing power and duration of the training process.

Nowadays, when creating datasets, AI labs don't limit themselves to human-made books, codeâ€”they use current models to generate artificial (synthetic) data based on evaluated needs.

With the goal of getting correct pattern match to user's question, generating synthetic data is mostly about talking about the same things in many different ways.

Perception of the model's intelligence is a brilliant illusion made when it is able to match pattern seen in its dataset.

## What is agent?

## Why CWC is non-agentic?
